
"""
    gradient!(grad, Î¸, model)

Compute gradient of negative log posterior with respect to Î¸ in-place.
"""
function gradient!(grad, Î¸, model::SegmentedModel)
    n = length(model.y)
    n_beta = model.n_breakpoints + 2

    Î² = Î¸[1:n_beta]
    Ïˆ = Î¸[n_beta+1:n_beta+model.n_breakpoints]
    log_Ïƒ = Î¸[end]
    Ïƒ = exp(log_Ïƒ)
    Ïƒ2 = Ïƒ^2

    x = model.x
    
    # Compute predictions and residuals
    yÌ‚ = predict(model, Î¸)
    residuals = model.y .- yÌ‚

    sum_sq_residuals = sum(residuals.^2)
    
    # Gradient w.r.t. Î²
    # âˆ‚NLL/âˆ‚Î²[1] = -1/ÏƒÂ² * Î£ residuals * 1
    grad[1] = -sum(residuals) / Ïƒ2
    
    # âˆ‚NLL/âˆ‚Î²[2] = -1/ÏƒÂ² * Î£ residuals * x
    grad[2] = -sum(residuals .* x) / Ïƒ2
    
    # âˆ‚NLL/âˆ‚Î²[i+2] = -1/ÏƒÂ² * Î£ residuals * max(0, x - Ïˆ[i])
    for i in 1:model.n_breakpoints
        grad[i+2] = -sum(residuals .* max.(0, x .- Ïˆ[i])) / Ïƒ2
    end
    
    # Gradient w.r.t. Ïˆ
    # âˆ‚NLL/âˆ‚Ïˆ[i] = Î²[i+2]/ÏƒÂ² * Î£ residuals * ðŸ™(x > Ïˆ[i])
    for i in 1:model.n_breakpoints
        indicators = Float64.(x .> Ïˆ[i])
        grad[n_beta + i] = Î²[i+2] / Ïƒ2 * sum(residuals .* indicators)
    end
    
    # Gradient w.r.t. log_Ïƒ
    # âˆ‚NLL/âˆ‚log_Ïƒ = n - sum_sq_residuals/ÏƒÂ²
    grad[end] = n - sum_sq_residuals / Ïƒ2
    
    return grad
end