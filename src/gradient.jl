
"""
    gradient!(grad, Î¸, model)

Compute gradient of negative log posterior with respect to Î¸ in-place.
"""
function gradient!(grad, Î¸, model::SegmentedModel)
    n = length(model.y)
    n_beta = model.n_breakpoints + 2

    Î² = Î¸[1:n_beta]
    Î¸_Ïˆ = Î¸[n_beta+1:n_beta+model.n_breakpoints]
    Ïˆ = transform_to_ordered(Î¸_Ïˆ)  # Transform to ordered space
    log_Ïƒ = Î¸[end]
    Ïƒ = exp(log_Ïƒ)
    Ïƒ2 = Ïƒ^2

    x = model.x

    # Compute predictions and residuals
    Å· = predict(model, Î¸)
    residuals = model.y .- Å·

    sum_sq_residuals = sum(residuals.^2)

    # Get prior parameters
    mean_y = mean(model.y)
    sd_y = std(model.y)

    # Gradient w.r.t. Î² (likelihood + prior)
    # Likelihood: âˆ‚NLL/âˆ‚Î²[1] = -1/ÏƒÂ² * Î£ residuals * 1
    # Prior: beta[1] ~ normal(mean_y, sd_y*2), âˆ‚/âˆ‚Î²[1] = (Î²[1] - mean_y)/(sd_y*2)^2
    prior_var_intercept = (sd_y * 2)^2
    grad[1] = -sum(residuals) / Ïƒ2 + (Î²[1] - mean_y) / prior_var_intercept

    # âˆ‚NLL/âˆ‚Î²[2] = -1/ÏƒÂ² * Î£ residuals * x
    # Prior: beta[2] ~ normal(0, 10), âˆ‚/âˆ‚Î²[2] = Î²[2]/100
    grad[2] = -sum(residuals .* x) / Ïƒ2 + Î²[2] / 100.0

    # âˆ‚NLL/âˆ‚Î²[i+2] = -1/ÏƒÂ² * Î£ residuals * max(0, x - Ïˆ[i])
    # Prior: beta[i+2] ~ normal(0, 10), âˆ‚/âˆ‚Î²[i+2] = Î²[i+2]/100
    for i in 1:model.n_breakpoints
        grad[i+2] = -sum(residuals .* max.(0, x .- Ïˆ[i])) / Ïƒ2 + Î²[i+2] / 100.0
    end

    # Gradient w.r.t. Ïˆ (in ordered space)
    # âˆ‚NLL/âˆ‚Ïˆ[i] = Î²[i+2]/ÏƒÂ² * Î£ residuals * ðŸ™(x > Ïˆ[i])
    grad_Ïˆ = Vector{Float64}(undef, model.n_breakpoints)
    for i in 1:model.n_breakpoints
        indicators = Float64.(x .> Ïˆ[i])
        grad_Ïˆ[i] = Î²[i+2] / Ïƒ2 * sum(residuals .* indicators)
    end

    # Transform gradient to unconstrained space using chain rule
    # Ïˆ[1] = Î¸_Ïˆ[1], Ïˆ[i] = Ïˆ[i-1] + exp(Î¸_Ïˆ[i])
    # âˆ‚Ïˆ[j]/âˆ‚Î¸_Ïˆ[1] = 1 for all j
    # âˆ‚Ïˆ[j]/âˆ‚Î¸_Ïˆ[i] = exp(Î¸_Ïˆ[i]) for all j â‰¥ i (i > 1)
    for i in 1:model.n_breakpoints
        if i == 1
            # âˆ‚L/âˆ‚Î¸_Ïˆ[1] = Î£_j âˆ‚L/âˆ‚Ïˆ[j] * âˆ‚Ïˆ[j]/âˆ‚Î¸_Ïˆ[1] = Î£_j âˆ‚L/âˆ‚Ïˆ[j]
            grad[n_beta + i] = sum(grad_Ïˆ)
        else
            # âˆ‚L/âˆ‚Î¸_Ïˆ[i] = Î£_{jâ‰¥i} âˆ‚L/âˆ‚Ïˆ[j] * exp(Î¸_Ïˆ[i])
            # Add Jacobian gradient: âˆ‚/âˆ‚Î¸_Ïˆ[i][log|det(J)|] = 1
            # Since we subtract log|det(J)| from NLL, we subtract 1 from gradient
            grad[n_beta + i] = sum(grad_Ïˆ[i:end]) * exp(Î¸_Ïˆ[i]) - 1.0
        end
    end

    # Gradient w.r.t. log_Ïƒ
    # Likelihood: âˆ‚NLL/âˆ‚log_Ïƒ = n - sum_sq_residuals/ÏƒÂ²
    # Prior: sigma ~ exponential(10), âˆ‚/âˆ‚log_Ïƒ[10*Ïƒ] = 10*Ïƒ
    grad[end] = n - sum_sq_residuals / Ïƒ2 + 10.0 * Ïƒ

    return grad
end
